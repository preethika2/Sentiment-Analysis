#Make sure to use virtual environment to load importation statements
import pandas as pd
from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import classification_report

# Load in sample data set
data = pd.read_csv('/Users/ryan/Desktop/Sentiment_Analysis/Product_CSVs/culturelleWomensSupplement.csv')

# Remove any rows that do not have a review
data = data.dropna(subset=['Review'])

# Assign TextBlob sentiment to the CSV file to get a sentiment review to compare against our machine learning algo
data['textblob_sentiment'] = data['Review'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

# Convert sentiment scores to sentiment labels useful to us
data['sentiment_label'] = data['textblob_sentiment'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))

# Vectorize the data
vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer.fit_transform(data['Review'])

# Train the data
clf = SVC(kernel='linear')
clf.fit(X_tfidf, data['sentiment_label'])

# Predict sentiments for the entire dataset given data
data['predicted_sentiment_label'] = clf.predict(X_tfidf)
data = data.drop(columns=['textblob_sentiment'])

# Save the predicted sentiments to a CSV file
data.to_csv('/Users/ryan/Desktop/Sentiment_Analysis/predicted_sentiments.csv', index=False)

# Calculate accuracy
accuracy = accuracy_score(data['sentiment_label'], data['predicted_sentiment_label'])

# Calculate precision
precision = precision_score(data['sentiment_label'], data['predicted_sentiment_label'], average='weighted')

# Calculate recall
recall = recall_score(data['sentiment_label'], data['predicted_sentiment_label'], average='weighted')

# Calculate confusion matrix
conf_matrix = confusion_matrix(data['sentiment_label'], data['predicted_sentiment_label'])

# Cross-Validation Scores
scores = cross_val_score(clf, X_tfidf, data['sentiment_label'], cv=5)

# Perform Grid Search for hyperparameter tuning
param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid_search = GridSearchCV(clf, param_grid, cv=5)
grid_search.fit(X_tfidf, data['sentiment_label'])
print("Best Parameters:", grid_search.best_params_)  # specified grid of hyperparameter values for the best combination.

# Print Classification Report
print("Classification Report:")
print(classification_report(data['sentiment_label'], data['predicted_sentiment_label']))

print("Cross-Validation Mean Accuracy:", scores.mean())  # average accuracy of a model
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("Confusion Matrix:")
print(conf_matrix)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['negative', 'neutral', 'positive'],
            yticklabels=['negative', 'neutral', 'positive'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png')
plt.show()

# Reading new csv file generated by us
data = pd.read_csv('predicted_sentiments.csv')

# Count occurrences of each sentiment label in both columns
sentiment_counts = data['sentiment_label'].value_counts()
predicted_sentiment_counts = data['predicted_sentiment_label'].value_counts()

# Plotting
plt.figure(figsize=(12, 6)) 
bar_width = 0.35  
sns.countplot(data=data, x='sentiment_label', palette='pastel', label='TextBlob', order=['positive', 'neutral', 'negative'], alpha=0.7, zorder=3, width=bar_width)
sns.countplot(data=data, x='predicted_sentiment_label', palette='muted', label='Machine Learning', order=['positive', 'neutral', 'negative'], alpha=0.7, zorder=2, width=bar_width)
plt.title('Sentiment Comparison: TextBlob vs. ML Algorithm')
plt.xlabel('Sentiment')
plt.ylabel('Count')

# Making sure the bars are side by side
for i, p in enumerate(plt.gca().patches[len(sentiment_counts):]):
    p.set_x(p.get_x() + bar_width)
    height = p.get_height()
    plt.text(p.get_x() + p.get_width() / 2, height + 0.1, predicted_sentiment_counts[i], ha="center", fontsize=10)

# Adding count labels to each bar
for i, p in enumerate(plt.gca().patches[:len(sentiment_counts)]):
    height = p.get_height()
    plt.text(p.get_x() + p.get_width() / 2, height + 0.1, sentiment_counts[i], ha="center", fontsize=10)

plt.grid(axis='y', linestyle='--', alpha=0.7, zorder=1)  # Add grid lines
plt.legend()
plt.tight_layout()
plt.savefig('sentiment_comparison_chart.png')
plt.show()

# Print counts
print("TextBlob Sentiment Counts:")
print(sentiment_counts)
print("\nMachine Learning Sentiment Counts:")
print(predicted_sentiment_counts)



